{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(y.shape)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and standarization of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 19)\n"
     ]
    }
   ],
   "source": [
    "def clean_tx(tx, threshold):\n",
    "    row, col = tx.shape[0], tx.shape[1]\n",
    "    empty = np.zeros(col)\n",
    "    for i in range(col):\n",
    "        count = np.count_nonzero(tx[:,i] == -999)\n",
    "        empty[i] = count/row    \n",
    "    \n",
    "    return tx[:,(empty <= threshold)]\n",
    "        \n",
    "print(clean_tx(tX, 0.15).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    return (x - mean_x) / std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tX = clean_tx(tX, 0.15)\n",
    "std_tX = standardize(clean_tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = std_tX.shape[0]\n",
    "model_data = np.c_[np.ones(row), std_tX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_y = np.where(y == -1, 0, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(model_data.shape[1])\n",
    "gamma = 0.7\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/99): loss=58.09307362488482\n",
      "Stochastic Gradient Descent(1/99): loss=573.4605838780876\n",
      "Stochastic Gradient Descent(2/99): loss=4508.326370049001\n",
      "Stochastic Gradient Descent(3/99): loss=113277.33810345783\n",
      "Stochastic Gradient Descent(4/99): loss=3837.1114137663158\n",
      "Stochastic Gradient Descent(5/99): loss=59815.58030304029\n",
      "Stochastic Gradient Descent(6/99): loss=319477.0153872045\n",
      "Stochastic Gradient Descent(7/99): loss=630024.5558809399\n",
      "Stochastic Gradient Descent(8/99): loss=301379.67805480555\n",
      "Stochastic Gradient Descent(9/99): loss=22449361.06468936\n",
      "Stochastic Gradient Descent(10/99): loss=32241410.263458643\n",
      "Stochastic Gradient Descent(11/99): loss=1043376602.8759407\n",
      "Stochastic Gradient Descent(12/99): loss=873055059.6739109\n",
      "Stochastic Gradient Descent(13/99): loss=2985145246.767544\n",
      "Stochastic Gradient Descent(14/99): loss=9024491665.861437\n",
      "Stochastic Gradient Descent(15/99): loss=350702098.8435756\n",
      "Stochastic Gradient Descent(16/99): loss=23447558.99928449\n",
      "Stochastic Gradient Descent(17/99): loss=375399335392.30566\n",
      "Stochastic Gradient Descent(18/99): loss=31581686151663.832\n",
      "Stochastic Gradient Descent(19/99): loss=145767675694.5398\n",
      "Stochastic Gradient Descent(20/99): loss=11596472498148.826\n",
      "Stochastic Gradient Descent(21/99): loss=306637914295056.9\n",
      "Stochastic Gradient Descent(22/99): loss=65756918554872.14\n",
      "Stochastic Gradient Descent(23/99): loss=1698423320214937.5\n",
      "Stochastic Gradient Descent(24/99): loss=223767028390379.12\n",
      "Stochastic Gradient Descent(25/99): loss=206656977952342.28\n",
      "Stochastic Gradient Descent(26/99): loss=6893809909354902.0\n",
      "Stochastic Gradient Descent(27/99): loss=4317187332687252.5\n",
      "Stochastic Gradient Descent(28/99): loss=1711913680945177.0\n",
      "Stochastic Gradient Descent(29/99): loss=1474507236474668.8\n",
      "Stochastic Gradient Descent(30/99): loss=556750313163608.5\n",
      "Stochastic Gradient Descent(31/99): loss=1.5873073824219248e+16\n",
      "Stochastic Gradient Descent(32/99): loss=1.8180799534095757e+17\n",
      "Stochastic Gradient Descent(33/99): loss=1.56878126148787e+19\n",
      "Stochastic Gradient Descent(34/99): loss=3.673688225103688e+20\n",
      "Stochastic Gradient Descent(35/99): loss=1.1066094342432876e+20\n",
      "Stochastic Gradient Descent(36/99): loss=1.0935524169568685e+21\n",
      "Stochastic Gradient Descent(37/99): loss=1.1515138280353004e+22\n",
      "Stochastic Gradient Descent(38/99): loss=8.563710027710024e+22\n",
      "Stochastic Gradient Descent(39/99): loss=4.4437474606105716e+20\n",
      "Stochastic Gradient Descent(40/99): loss=6.275015497381276e+24\n",
      "Stochastic Gradient Descent(41/99): loss=3.03019100451909e+25\n",
      "Stochastic Gradient Descent(42/99): loss=2.8588653586061888e+28\n",
      "Stochastic Gradient Descent(43/99): loss=1.329621661568443e+29\n",
      "Stochastic Gradient Descent(44/99): loss=2.289829404775259e+30\n",
      "Stochastic Gradient Descent(45/99): loss=4.4051473309272274e+33\n",
      "Stochastic Gradient Descent(46/99): loss=1.1041725573590361e+34\n",
      "Stochastic Gradient Descent(47/99): loss=9.4389421160637e+32\n",
      "Stochastic Gradient Descent(48/99): loss=1.152475195003335e+34\n",
      "Stochastic Gradient Descent(49/99): loss=5.55214259973683e+33\n",
      "Stochastic Gradient Descent(50/99): loss=2.950406942094388e+35\n",
      "Stochastic Gradient Descent(51/99): loss=1.280276525921617e+36\n",
      "Stochastic Gradient Descent(52/99): loss=6.634082707776619e+36\n",
      "Stochastic Gradient Descent(53/99): loss=2.2074045996296806e+37\n",
      "Stochastic Gradient Descent(54/99): loss=1.3302829749989983e+39\n",
      "Stochastic Gradient Descent(55/99): loss=1.1946909544782615e+43\n",
      "Stochastic Gradient Descent(56/99): loss=7.894681207430154e+41\n",
      "Stochastic Gradient Descent(57/99): loss=8.8780445052594e+41\n",
      "Stochastic Gradient Descent(58/99): loss=1.3855064384628207e+42\n",
      "Stochastic Gradient Descent(59/99): loss=2.647700136097791e+43\n",
      "Stochastic Gradient Descent(60/99): loss=5.502293352689778e+43\n",
      "Stochastic Gradient Descent(61/99): loss=2.3565067227068267e+44\n",
      "Stochastic Gradient Descent(62/99): loss=5.025814384849541e+45\n",
      "Stochastic Gradient Descent(63/99): loss=9.198241350587652e+47\n",
      "Stochastic Gradient Descent(64/99): loss=1.2154139518174415e+48\n",
      "Stochastic Gradient Descent(65/99): loss=3.152852803876557e+50\n",
      "Stochastic Gradient Descent(66/99): loss=1.0498328848811619e+51\n",
      "Stochastic Gradient Descent(67/99): loss=7.081962849987913e+51\n",
      "Stochastic Gradient Descent(68/99): loss=6.883505396303663e+51\n",
      "Stochastic Gradient Descent(69/99): loss=8.83665580872676e+48\n",
      "Stochastic Gradient Descent(70/99): loss=5.536446003437934e+52\n",
      "Stochastic Gradient Descent(71/99): loss=7.764565588832062e+54\n",
      "Stochastic Gradient Descent(72/99): loss=4.18558187631873e+53\n",
      "Stochastic Gradient Descent(73/99): loss=4.9958693644130205e+54\n",
      "Stochastic Gradient Descent(74/99): loss=2.6691053312544735e+54\n",
      "Stochastic Gradient Descent(75/99): loss=5.5254972279487926e+57\n",
      "Stochastic Gradient Descent(76/99): loss=1.656824718975052e+58\n",
      "Stochastic Gradient Descent(77/99): loss=2.5508997968041616e+60\n",
      "Stochastic Gradient Descent(78/99): loss=2.3694547462771716e+61\n",
      "Stochastic Gradient Descent(79/99): loss=4.4125397042595427e+64\n",
      "Stochastic Gradient Descent(80/99): loss=3.415628622902685e+64\n",
      "Stochastic Gradient Descent(81/99): loss=2.990719186683997e+64\n",
      "Stochastic Gradient Descent(82/99): loss=6.288280376911372e+61\n",
      "Stochastic Gradient Descent(83/99): loss=5.576188113468149e+65\n",
      "Stochastic Gradient Descent(84/99): loss=2.251489954617202e+66\n",
      "Stochastic Gradient Descent(85/99): loss=6.830117671858632e+67\n",
      "Stochastic Gradient Descent(86/99): loss=1.0391634634078033e+69\n",
      "Stochastic Gradient Descent(87/99): loss=4.833290924652624e+69\n",
      "Stochastic Gradient Descent(88/99): loss=2.3825419449191957e+70\n",
      "Stochastic Gradient Descent(89/99): loss=1.2545225825500756e+71\n",
      "Stochastic Gradient Descent(90/99): loss=4.01827755895075e+71\n",
      "Stochastic Gradient Descent(91/99): loss=7.981963006967179e+71\n",
      "Stochastic Gradient Descent(92/99): loss=1.605997111068162e+72\n",
      "Stochastic Gradient Descent(93/99): loss=4.629651898337108e+71\n",
      "Stochastic Gradient Descent(94/99): loss=9.863285641335485e+72\n",
      "Stochastic Gradient Descent(95/99): loss=3.0744134820099805e+73\n",
      "Stochastic Gradient Descent(96/99): loss=1.0495436686275188e+75\n",
      "Stochastic Gradient Descent(97/99): loss=2.0745436318007663e+76\n",
      "Stochastic Gradient Descent(98/99): loss=8.025923198550652e+75\n",
      "Stochastic Gradient Descent(99/99): loss=2.916512589639328e+77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 6.21018206e+37],\n",
       "        [ 3.43355266e+37],\n",
       "        [-1.80582317e+37],\n",
       "        [-4.16438683e+37],\n",
       "        [-1.46138710e+37],\n",
       "        [-2.06904252e+37],\n",
       "        [-3.77905647e+37],\n",
       "        [ 1.84761256e+37],\n",
       "        [-3.18490000e+37],\n",
       "        [-2.54143238e+37],\n",
       "        [-7.24006731e+37],\n",
       "        [-1.92296985e+36],\n",
       "        [-1.61175212e+36],\n",
       "        [-8.69293661e+37],\n",
       "        [-7.41157630e+37],\n",
       "        [-6.82152543e+36],\n",
       "        [ 2.00451494e+37],\n",
       "        [-4.05825689e+37],\n",
       "        [-4.41870115e+37],\n",
       "        [-3.84090473e+37]]),\n",
       " 2.916512589639328e+77)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    loss = 0\n",
    "    for i in range(max_iters):\n",
    "        for batch_y, batch_x in batch_iter(y, tx):\n",
    "            gradient,_ = compute_gradient(batch_y, batch_x, w)\n",
    "            w = w - gamma*gradient\n",
    "            loss = compute_mse(y - (batch_x @ w))\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=i, ti=max_iters - 1, l=loss))\n",
    "    return w, loss\n",
    "\n",
    "least_squares_SGD(y, model_data, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(model_data.shape[1])\n",
    "gamma = 0.7\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size=1, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    sig = sigmoid(tx @ w)\n",
    "    loss = y.T @ np.log(sig) + (1 - y).T @ np.log(1 - sig)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_computation(y, tx, w):\n",
    "    sig = sigmoid(tx @ w)\n",
    "    grad = tx.T @ (sig.flatten() - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(y, tx, w, gamma):\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = gradient_computation(y, tx, w)\n",
    "    w = w.flatten() - gamma*grad\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.896730967031678e-07\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    threshold = 1e-8\n",
    "    for iter in range(max_iters):\n",
    "        for batch_y, batch_x in batch_iter(y, tx):\n",
    "            w, loss = regression(batch_y, batch_x, w, gamma)\n",
    "        \n",
    "        \n",
    "        #if iter % 100 == 0:\n",
    "           # print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "            losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "    # visualization\n",
    "    #visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_gradient_descent\")\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, losses[-1]\n",
    "        \n",
    "initial_w = np.zeros((model_data.shape[1], 1))\n",
    "w_final, loss_final = logistic_regression(model_y, model_data, initial_w, max_iters, gamma)\n",
    "print(loss_final)\n",
    "#print(model_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction(y, tx, w):\n",
    "    pred = sigmoid(tx @ w)\n",
    "    pred = np.where(pred > 0.5, 1, pred)\n",
    "    pred = np.where(pred <= 0.5, 0, pred)\n",
    "    count = 0\n",
    "    for i, j in zip(y, pred):\n",
    "        if i == j:\n",
    "            count += 1\n",
    "    return count/len(y)\n",
    "    \n",
    "prediction(model_y, model_data, w_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
