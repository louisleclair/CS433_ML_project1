{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import implementations\n",
    "import helpers\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(y.shape)\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and standarization of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.15\n",
    "all_tx, all_y = helpers.build_data(y, tX, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tX = helpers.clean_tx(tX, 0.15)\n",
    "std_tX = helpers.standardize(clean_tX)\n",
    "row = std_tX.shape[0]\n",
    "model_data = np.c_[np.ones(row), std_tX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do your thing crazy machine learning thing here :) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(model_data.shape[1])\n",
    "gamma = 0.01\n",
    "max_iters = 300\n",
    "lambda_ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2992326  -0.21957221 -0.05760512  0.06179666  0.12080422 -0.05052733\n",
      "  0.02068824 -0.08682256  0.1348509   0.13329037 -0.00137684 -0.0025637\n",
      "  0.06636619 -0.00074148  0.00253707  0.01658314  0.00179607 -0.02090431\n",
      "  0.00766294 -0.02099584] 0.36210667089730614\n",
      "0.723796\n"
     ]
    }
   ],
   "source": [
    "w, loss = implementations.least_square_GD(y, model_data, initial_w, max_iters, gamma)\n",
    "print(w, loss)\n",
    "tmp = y == predict_labels(w, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31117809 -0.24709017 -0.08969226  0.11493254  0.13271232 -0.05327159\n",
      "  0.0866703   0.07194741  0.18004492  0.0367816  -0.006251    0.02718262\n",
      "  0.0762358  -0.01958307 -0.02983458 -0.00608005  0.06051118 -0.00625945\n",
      "  0.02182407  0.07673993] 0.1645397727593384\n",
      "0.682708\n"
     ]
    }
   ],
   "source": [
    "w, loss = implementations.least_square_SGD(y, model_data, initial_w, max_iters, gamma)\n",
    "print(w, loss)\n",
    "tmp = y == predict_labels(w, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.14664000e-01 -2.61046202e-01 -2.60495896e-01  2.18600813e-03\n",
      "  2.84091422e-01 -4.08633500e-02 -3.60254350e+02 -1.93462667e-01\n",
      "  1.33047784e-01  6.99737217e+01 -9.06834519e-04 -8.61974632e-04\n",
      "  6.89948892e+01 -5.98583316e-04  2.31205241e-03  1.08372889e-01\n",
      "  8.37090896e-04 -7.03488098e-02  8.36189386e-02  3.05118660e+02] 0.35251271029898895\n",
      "0.73356\n"
     ]
    }
   ],
   "source": [
    "w, loss = implementations.least_square(y, model_data)\n",
    "print(w, loss)\n",
    "tmp = y == predict_labels(w, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26222    -0.20226141 -0.08383322  0.04792501  0.13147698 -0.0408149\n",
      "  0.02176297 -0.09289289  0.11972484  0.1317687  -0.0011034  -0.00200653\n",
      "  0.08737519 -0.00056122  0.00224169  0.02863349  0.0015625  -0.02559142\n",
      "  0.02186715 -0.02410863] 0.36257036691673894\n",
      "0.726424\n"
     ]
    }
   ],
   "source": [
    "w, loss = implementations.ridge_regression(y, model_data, lambda_)\n",
    "print(w, loss)\n",
    "tmp = y == predict_labels(w, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.87323399e+00 -3.52547965e-01 -2.43545244e-01  2.02169028e-01\n",
      "  1.59490471e-03 -3.79791710e-01  2.41255210e-01 -2.30879062e-01\n",
      "  3.91687125e-01  2.45400008e-01  2.54048954e-01 -1.78613377e-01\n",
      "  6.92948436e-02  2.54387707e-01  6.54765833e-02  1.40053927e-01\n",
      " -1.49373093e-01  1.00753657e-01  1.48191673e-01  2.13086194e-01] 1.305518724957287\n",
      "0.67544\n"
     ]
    }
   ],
   "source": [
    "w_final, loss_final = implementations.logistic_regression(y, model_data, initial_w, max_iters, gamma)\n",
    "print(w_final,loss_final)\n",
    "tmp = y == predict_labels(w_final, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.27903735 -0.45183278  0.14902619  0.23402499 -0.00367915 -0.12349203\n",
      "  0.23428769 -0.39292466  0.37454519  0.51603803 -0.11286803 -0.18927387\n",
      "  0.05038865 -0.26159962 -0.02583562 -0.00721014  0.08826443  0.13539957\n",
      " -0.03452798  0.14723233] 0.9140614985863633\n",
      "0.688768\n"
     ]
    }
   ],
   "source": [
    "w_final, loss_final = implementations.reg_logistic_regression(y, model_data, initial_w, max_iters, gamma, lambda_)\n",
    "print(w_final,loss_final)\n",
    "tmp = y == predict_labels(w_final, model_data)\n",
    "print(np.count_nonzero(tmp == 1)/len(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main code we submit, above is just test with a basic cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Gamma and Lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894379941947752 0.2395026619987486\n",
      "0.789270343308978 0.2395026619987486\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-2, 0,num=30)\n",
    "k_fold = 5\n",
    "seed = 0\n",
    "max_iters = 100\n",
    "gamma = 1e-5\n",
    "\n",
    "wanted_index = 0\n",
    "\n",
    "tx_wanted = all_tx[wanted_index]\n",
    "y_wanted = all_y[wanted_index]\n",
    "\n",
    "k_indices = helpers.build_k_indices(y_wanted, k_fold, seed)\n",
    "initial_w = np.zeros(tx_wanted.shape[1])\n",
    "\n",
    "arr_pred_tr = []\n",
    "arr_pred_te = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    pred_tr_tmp = []\n",
    "    pred_te_tmp = []\n",
    "    for k in range(k_fold):\n",
    "        pred_tr, pred_te = helpers.cross_validation(y_wanted, tx_wanted, initial_w, max_iters, gamma, lambda_, k_indices, k)\n",
    "        pred_tr_tmp.append(pred_tr)\n",
    "        pred_te_tmp.append(pred_te)\n",
    "    arr_pred_tr.append(np.mean(pred_tr_tmp))\n",
    "    arr_pred_te.append(np.mean(pred_te_tmp))\n",
    "    \n",
    "maxIndex_tr = np.argmax(arr_pred_tr)\n",
    "maxIndex_te = np.argmax(arr_pred_te)\n",
    "    \n",
    "print(max(arr_pred_tr), lambdas[maxIndex_tr])\n",
    "print(max(arr_pred_te), lambdas[maxIndex_te])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e-5\n",
    "max_iter = 300\n",
    "all_w = []\n",
    "all_pred = []\n",
    "for tx, y in zip(all_tx, all_y):\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    w_final, _ = implementations.logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    all_pred.append(np.count_nonzero((y == predict_labels(w_final, tx)) == 1)/len(tmp))\n",
    "    all_w.append(w_final)\n",
    "    \n",
    "weights = all_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tx_te, all_id_te = helpers.build_data(ids_test, tX_test, 0.15)\n",
    "y_pred = helpers.pred_labels(all_tx_te, all_id_te, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
